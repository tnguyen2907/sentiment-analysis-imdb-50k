{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis with IMDB Dataset of 50K Movie Reviews\n",
        "\n",
        "[Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
      ],
      "metadata": {
        "id": "cFowCKTzJRWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview: This project has three parts\n",
        "- Part 1: Cleaning the data\n",
        "- Part 2: Classifying sentiments using Bag of Words (BOW) and Term Frequencies - Inverse Document Frequencies (TFIDF) as text encoder and Naive Bayes and Support Vector Machine as models\n",
        "- Part 3: Classifying sentiments using Deep learning (Traditional Neural Network and Bidirectional LSTM)\n",
        "\n",
        "Basically, my idea is starting with simple models and increase the complexity to see if more complex models would perform better\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xnAAUWCYKo8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's import the necessary libraries"
      ],
      "metadata": {
        "id": "88AQMHEuMDS-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Czh-ytCiF3aN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7224e526-abd3-49bf-a71d-76b7c4dcd262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D, LSTM, Bidirectional, Embedding, TextVectorization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "np.random.seed(69)\n",
        "tf.random.set_seed(69)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And read the dataset"
      ],
      "metadata": {
        "id": "S1wzS2F7MZPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "TSwag0blGVpW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "da729d00-1fed-4c05-c377-0a77ca4ea3e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  positive\n",
              "freq                                                    5     25000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-816a3e46-e624-4763-9d53-5200ba7130e3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-816a3e46-e624-4763-9d53-5200ba7130e3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-816a3e46-e624-4763-9d53-5200ba7130e3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-816a3e46-e624-4763-9d53-5200ba7130e3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cbb18613-e34b-4bc7-be94-78989395b2a7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbb18613-e34b-4bc7-be94-78989395b2a7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cbb18613-e34b-4bc7-be94-78989395b2a7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the description, we can see that this dataset has 50000 reviews, half of which is postive and the other is negative.\n",
        "\n",
        "There are duplicates but I don't think the cause big issues in this case so I decided to not remove them."
      ],
      "metadata": {
        "id": "WzdZd-WrrLs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpTvpNg-qtYs",
        "outputId": "c7e52965-9e00-49aa-c0b5-a91c1bc8d62b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "review       False\n",
              "sentiment    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luckily, there is no missing data in the dataset\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4elAkaxGr2ES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Clean the dataset"
      ],
      "metadata": {
        "id": "yUkoUtO6Mpmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a peek at some of the reviews"
      ],
      "metadata": {
        "id": "A_QARbtHr7ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for review in df['review'].iloc[:10]:\n",
        "    print(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EOS4Ch9sAp8",
        "outputId": "a700d0d5-2f14-4de1-97de-e93a53dd28ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n",
            "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.\n",
            "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n",
            "Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei's direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.\n",
            "Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. It just never gets old, despite my having seen it some 15 or more times in the last 25 years. Paul Lukas' performance brings tears to my eyes, and Bette Davis, in one of her very few truly sympathetic roles, is a delight. The kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. And the mother's slow awakening to what's happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they'd all be \"up\" for this movie.\n",
            "I sure would like to see a resurrection of a up dated Seahunt series with the tech they have today it would bring back the kid excitement in me.I grew up on black and white TV and Seahunt with Gunsmoke were my hero's every week.You have my vote for a comeback of a new sea hunt.We need a change of pace in TV and this would work for a world of under water adventure.Oh by the way thank you for an outlet like this to view many viewpoints about TV and the many movies.So any ole way I believe I've got what I wanna say.Would be nice to read some more plus points about sea hunt.If my rhymes would be 10 lines would you let me submit,or leave me out to be in doubt and have me to quit,If this is so then I must go so lets do it.\n",
            "This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.\n",
            "Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\n",
            "If you like original gut wrenching laughter you will like this movie. If you are young or old then you will love this movie, hell even my mom liked it.<br /><br />Great Camp!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the texts, I come up with a few steps to prepare our data for training *(look at the ```clean_text``` function to see how this works)*:\n",
        "- First, standardize by making everything into lowercase since it will result in a lower number of tokens\n",
        "- Remove all the HTML tags (I skimmed through a big chunk of the dataset and this seems to be the only weird things)\n",
        "- Tokenize the texts using NLTK tokenizer\n",
        "- Remove all the punctuation and symbols (symbols can indicate sentiments but I believe the effect is negligible in this task)\n",
        "- Let's also remove all words with just one character because usually they don't indicate sentiments (words like \"I\") and remove all the new word appearing from the last step (\"e.g\" -> \"e g\")\n",
        "- Then, remove all the stop words because they don't indicate sentiments. Note that I used the stop words list from NLTK and there are some words in that list that I want to keep (words that can indicate sentiments and negation)\n",
        "- Finally, lemmatize all the words to, again, standardize and reduce the number of tokens"
      ],
      "metadata": {
        "id": "uYAI5S8rsjCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop words but not those that are important\n",
        "def is_stop_word(text):\n",
        "    important_stop_words = [\"n't\", \"no\", \"nor\", \"not\", \"never\", \"against\", 'above', 'below', 'up', 'down', 'out', 'on', 'off', 'over', 'under', 'again']\n",
        "    return (text in stopwords.words(\"english\")) and (text not in important_stop_words)\n",
        "\n",
        "# A valid word is a word with length more than 1 or a number\n",
        "def is_valid_word(text):\n",
        "    return (text.isalpha() and len(text) > 1) or (text.isnumeric())\n",
        "\n",
        "def clean_text(row):\n",
        "    # Lowercase\n",
        "    row = row.lower()\n",
        "\n",
        "    # Get rid of the HTML tag\n",
        "    row = BeautifulSoup(row, \"html.parser\").get_text()\n",
        "\n",
        "    # First, tokenize the text\n",
        "    tokens = word_tokenize(row)\n",
        "\n",
        "    # Change all the \"n't\" into \"not\"\n",
        "    tokens = [\"not\" if token == \"n't\" else token for token in tokens]\n",
        "\n",
        "    #This is a packed line\n",
        "        # Get rid of invalid words\n",
        "            # All the symbols and punctuations (but keep \"n't\" since it's negation)\n",
        "            # All words of length 1\n",
        "            # All stop words but keep those that can express sentiments or negation (see list above)\n",
        "        #Lemmatize each word\n",
        "    row = \" \".join([WordNetLemmatizer().lemmatize(token) for token in tokens if is_valid_word(token) and not is_stop_word(token)])\n",
        "\n",
        "    return row\n"
      ],
      "metadata": {
        "id": "barXQ0ItGYB0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the data and save it to a file"
      ],
      "metadata": {
        "id": "fhOMye1AwyRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this to False to reprocess the data again\n",
        "has_preprocessed = True\n",
        "\n",
        "if has_preprocessed:\n",
        "    df['review'] = pd.read_csv(\"preprocessed_data.csv\").squeeze()\n",
        "else:\n",
        "    df['review'] = df['review'].apply(clean_text)\n",
        "    df['review'].to_csv(\"preprocessed_data.csv\", index=False)\n",
        "\n",
        "# Encoded the labels with 0, 1\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})"
      ],
      "metadata": {
        "id": "XEhWTEGpGM3U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split our data into a training dataset and a test dataset, and we are ready to train models"
      ],
      "metadata": {
        "id": "mmKFBK_xNFU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['review']\n",
        "y = df['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=69)"
      ],
      "metadata": {
        "id": "hz2ceCxqGtPk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "brvfHikSxRds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Using BOW and TFIDF as text encoder and Naive Bayes and SVM as models"
      ],
      "metadata": {
        "id": "VMu8F_I-NI7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Text Encoding\n",
        "We cannot use strings as the inputs to ML models so the first thing is encoding the texts. Let's use two classic ways to encode texts: Bag of Words (BOW) and Term Frequencies - Inverse Document Frequencies (TFIDF)\n",
        "- BOW creates features from texts using word frequencies\n",
        "- TFIDF also relies on word frequencies but adds a few more stuffs so it's more \"normalized\""
      ],
      "metadata": {
        "id": "oeSUaMIMNWfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow = CountVectorizer()\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "X_train_bow = bow.fit_transform(X_train)\n",
        "X_test_bow = bow.transform(X_test)\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ],
      "metadata": {
        "id": "u2EPFkSMNiXj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Models\n",
        "There are two classification models that are known to work really well with text data: Naive Bayes and Support Vector Machine. I believe the reason is that they work really well with sparse data (BOW and TFIDF features are sparse matrices)\n",
        "\n",
        "Also, let's train all four of the combinations since they are really quick to train"
      ],
      "metadata": {
        "id": "w7rAVS1pNn3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc_bow = LinearSVC().fit(X_train_bow, y_train)\n",
        "print(\"Linear Support Vector Machine with Bag of Words: \\n\", classification_report(y_test, svc_bow.predict(X_test_bow)))\n",
        "\n",
        "svc_tfidf = LinearSVC().fit(X_train_tfidf, y_train)\n",
        "print(\"Linear Support Vector Machine with TFIDF: \\n\", classification_report(y_test, svc_tfidf.predict(X_test_tfidf)))\n",
        "\n",
        "nb_bow = MultinomialNB().fit(X_train_bow, y_train)\n",
        "print(\"Naive Bayes with Bag of Words: \\n\", classification_report(y_test, nb_bow.predict(X_test_bow)))\n",
        "\n",
        "nb_tfidf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
        "print(\"Naive Bayes with TFIDF: \\n\", classification_report(y_test, nb_tfidf.predict(X_test_tfidf)))"
      ],
      "metadata": {
        "id": "gkSb2LjbHTGq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb6a271-2a4a-4791-f65d-5db0b626db9c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Support Vector Machine with Bag of Words: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87      4971\n",
            "           1       0.87      0.87      0.87      5029\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Linear Support Vector Machine with TFIDF: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.90      4971\n",
            "           1       0.89      0.90      0.90      5029\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n",
            "Naive Bayes with Bag of Words: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86      4971\n",
            "           1       0.87      0.83      0.85      5029\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n",
            "Naive Bayes with TFIDF: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86      4971\n",
            "           1       0.87      0.84      0.86      5029\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all four models yield pretty good results with Linear Support Vector Machine with TFIDF yielding the highest accuracy.\n",
        "\n",
        "I also notice there aren't much differences in precision and recall between the two labels. The models do not bias toward one label.\n",
        "\n",
        "--------------------------"
      ],
      "metadata": {
        "id": "8VZZyMC1OIlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Using Deep Learning\n",
        "\n",
        "Let's try to train more complex Deep Learning models to see if they perform better than previous models\n",
        "\n",
        "### A. Traditional Neural Network\n",
        "First, let's try a traditional neural network:\n",
        "- Neural networks don't work well with sparse data, so we cannot use BOW and TFIDF features from last steps. Let's use the TextVectorization layer from Tensorflow. It encodes each word with an integer\n",
        "- An Embedding layer\n",
        "- Two Dense layer with activation function 'relu' and dropout rate of 0.5 for regularization\n",
        "- And since this is binary classification, we have to use 'sigmoid' function for the last layer and 'binary_crossentropy' for the loss function\n",
        "\n",
        "(You're probably wondering why there is a GlobalAveragePooling1D layer. I tried to use the Flatten layer, and I didn't know why the loss did not improve. I probably missed something, so it didn't work. I did some research on StackOverflow and tried this GlobalAveragePooling1D layer. It actually worked really well)"
      ],
      "metadata": {
        "id": "CUeFufJfurZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model_nn = False\n",
        "\n",
        "nn = tf.keras.Model()\n",
        "\n",
        "if load_model_nn:\n",
        "    nn = tf.keras.models.load_model('nn.keras')\n",
        "else:\n",
        "    # Vectorize the strings\n",
        "    encoder1 = tf.keras.layers.TextVectorization()\n",
        "    encoder1.adapt(X)\n",
        "\n",
        "    nn = Sequential([\n",
        "        tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "        encoder1,\n",
        "        Embedding(len(encoder1.get_vocabulary()), 64, mask_zero=True),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(nn.summary())\n",
        "    nn.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.1 , callbacks=[EarlyStopping(patience=2)])\n",
        "\n",
        "    # nn.save('nn.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ya1gmn7IzCv",
        "outputId": "3fd65ea2-f3d5-459d-82bb-853737befa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVe  (None, None)              0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 64)          5747648   \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 64)                0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5750273 (21.94 MB)\n",
            "Trainable params: 5750273 (21.94 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "282/282 [==============================] - 29s 99ms/step - loss: 0.4751 - accuracy: 0.8022 - val_loss: 0.2572 - val_accuracy: 0.9032\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 28s 100ms/step - loss: 0.2368 - accuracy: 0.9254 - val_loss: 0.2586 - val_accuracy: 0.8995\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 28s 99ms/step - loss: 0.1581 - accuracy: 0.9567 - val_loss: 0.2881 - val_accuracy: 0.9010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the model overfits really quick. Observing this, I did try to add regularization. It's best to train the model for just 1 or 2 epochs, but I still let the model train for 3 epochs to demonstrate this."
      ],
      "metadata": {
        "id": "Cc_7f-r79QlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy score of Traditional Neural Network: \\n\", classification_report(y_test, tf.round(nn.predict(X_test), 0.5)))"
      ],
      "metadata": {
        "id": "XdYtL0Gwzin-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f05d29-8ae7-4b24-df04-807a66053963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step\n",
            "Accuracy score of Traditional Neural Network: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89      4971\n",
            "           1       0.88      0.91      0.89      5029\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of Neural Network (0.89) is about the same as that SVM with TFIDF (0.9)"
      ],
      "metadata": {
        "id": "bzNyRvmYcY53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Bi-LSTM\n",
        "Recurrent Neural Networks have been known for performing well with sequential data, such as texts, so let's try to use LSTM for our model. I also wrap the LSTM layer inside Bidirectional() so we would also look at each text backwards to identify any additional patterns. I read on Tensorflow website that Bi-LSTM tends to work really well with texts.\n",
        "\n",
        "The model consists of\n",
        "- Text Vectorization layer\n",
        "- A Bidirectional-LSTM layer\n",
        "- A Dense layer with activation function \"relu\"\n",
        "- Dropout for regularization\n",
        "- Final output layer with \"sigmoid\""
      ],
      "metadata": {
        "id": "ivxi61ncwbZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model_bilstm = False\n",
        "\n",
        "bilstm = tf.keras.Model()\n",
        "\n",
        "if load_model_bilstm:\n",
        "    bilstm = tf.keras.models.load_model('bilstm.keras')\n",
        "else:\n",
        "    # Vectorize the strings\n",
        "    encoder2 = tf.keras.layers.TextVectorization()\n",
        "    encoder2.adapt(X_train)\n",
        "\n",
        "    bilstm = Sequential([\n",
        "        tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "        encoder2,\n",
        "        Embedding(len(encoder2.get_vocabulary()), 64, mask_zero=True),\n",
        "        Bidirectional(LSTM(32, dropout=0.5)),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    bilstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print(bilstm.summary())\n",
        "    bilstm.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.1 , callbacks=[EarlyStopping(patience=2)])\n",
        "\n",
        "    # bilstm.save('bilstm.keras')"
      ],
      "metadata": {
        "id": "uHR1l5t_H9_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d1a554-86a6-41b6-d9d1-f63e415462ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (Text  (None, None)              0         \n",
            " Vectorization)                                                  \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 64)          5238848   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                24832     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5264737 (20.08 MB)\n",
            "Trainable params: 5264737 (20.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "282/282 [==============================] - 295s 1s/step - loss: 0.4199 - accuracy: 0.8101 - val_loss: 0.2536 - val_accuracy: 0.8985\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 278s 986ms/step - loss: 0.2022 - accuracy: 0.9312 - val_loss: 0.2639 - val_accuracy: 0.8972\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 280s 994ms/step - loss: 0.1254 - accuracy: 0.9606 - val_loss: 0.2831 - val_accuracy: 0.8945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can see that the model starts to overfits after the first few epochs"
      ],
      "metadata": {
        "id": "u-nptMwbfJy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy score of Bi-LSTM Neural Network: \\n\", classification_report(y_test, tf.round(bilstm.predict(X_test), 0.5)))"
      ],
      "metadata": {
        "id": "Duj5zFXy4Ms0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970e2281-ed6f-4c63-9dd0-bce472a61b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 28s 80ms/step\n",
            "Accuracy score of Bi-LSTM Neural Network: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.91      0.89      4971\n",
            "           1       0.91      0.86      0.88      5029\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.89      0.88      0.88     10000\n",
            "weighted avg       0.89      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turns out the model that usually works very well with texts does not perform any better than previous models.\n",
        "\n",
        "The accuracy of Bi-LSTM is 0.88, which is about the same as traditional NN (0.89) and SVM with TFIDF (0.9)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_lXlLxJGfg0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "- All three approaches (SVM with TFIDF, traditional NN, BI-LSTM) yield about the same accuracy ~0.89-0.9 (traditional NN and BI-LSTM perform a bit worse but I believe if I train them for fewer epochs, they would yield the same accuracy)\n",
        "\n",
        "- I expected Bi-LSTM to perform much better than other two approaches but that was not the case since it was \"the model\" for text classification. Maybe Bi-LSTM would perform better with more complex datasets.\n",
        "\n",
        "- I did a skim on Kaggle on how other practitioners did on this dataset and the accuracy is around 0.8 to 0.9, so our models perform pretty well compared to others. On Kaggle, deep Learning models are actually on the lower end and simpler models yield higher accuracy\n",
        "\n",
        "## Future works\n",
        "- During this project, I tried to train a Word Embedding model by Word2Vec and use it to extract features from texts, but the accuracy of the model is about 0.86 and the training time was hours so I decided to not include it in this notebook. In future, I may try to use word embeddings from pretrained models to see it performs better\n",
        "- Adding Attention machanism to Bi-LSTM maybe another thing we can try. Some words definitely indicate a sentiment more than others\n",
        "- Finally, we can try to add more rules-based approaches. Maybe put more weight on words that we know can indicate sentiments (like \"love\", \"hate\", \"1 out of 10\") and remove more words that aren't important (like \"film\", \"movies\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bd7UKrsqgOR1"
      }
    }
  ]
}